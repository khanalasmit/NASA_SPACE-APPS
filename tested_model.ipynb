{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0904de37",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.ensemble import BaggingClassifier,StackingClassifier,RandomForestClassifier,GradientBoostingClassifier\n",
    "from xgboost import XGBClassifier as XGC\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "38da4367",
   "metadata": {},
   "outputs": [],
   "source": [
    "Kepler_data=pd.read_csv('Training_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "8ff400db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 7326 entries, 0 to 7325\n",
      "Data columns (total 8 columns):\n",
      " #   Column           Non-Null Count  Dtype  \n",
      "---  ------           --------------  -----  \n",
      " 0   koi_period       7326 non-null   float64\n",
      " 1   koi_duration     7326 non-null   float64\n",
      " 2   koi_depth        7326 non-null   float64\n",
      " 3   koi_model_snr    7326 non-null   float64\n",
      " 4   koi_prad         7326 non-null   float64\n",
      " 5   koi_steff        7326 non-null   float64\n",
      " 6   koi_srad         7326 non-null   float64\n",
      " 7   koi_disposition  7326 non-null   object \n",
      "dtypes: float64(7), object(1)\n",
      "memory usage: 458.0+ KB\n"
     ]
    }
   ],
   "source": [
    "Kepler_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e77e7973",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['CONFIRMED' 'FALSE POSITIVE']\n"
     ]
    }
   ],
   "source": [
    "X=Kepler_data.drop(columns=['koi_disposition'])\n",
    "y=Kepler_data['koi_disposition']\n",
    "le=LabelEncoder()\n",
    "le.fit(y)\n",
    "y_trf=le.transform(y)\n",
    "print(le.classes_)\n",
    "X_train,X_test,y_train,y_test=train_test_split(X,y_trf,test_size=0.2,random_state=42,stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "db9a25af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin, clone\n",
    "\n",
    "class OOFStackingClassifier(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, base_models, meta_model, n_splits=5, random_state=42):\n",
    "        self.base_models = base_models\n",
    "        self.meta_model = meta_model\n",
    "        self.n_splits = n_splits\n",
    "        self.random_state = random_state\n",
    "        self.fitted_base_models = []  # will hold final trained clones of base models\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        X = np.asarray(X)\n",
    "        y = np.asarray(y)\n",
    "        n_samples = X.shape[0]\n",
    "\n",
    "        skf = StratifiedKFold(n_splits=self.n_splits, shuffle=True, random_state=self.random_state)\n",
    "        oof_preds = np.zeros((n_samples, len(self.base_models)))\n",
    "\n",
    "        # Out-of-fold predictions\n",
    "        for i, model in enumerate(self.base_models):\n",
    "            oof = np.zeros(n_samples)\n",
    "            for train_idx, val_idx in skf.split(X, y):\n",
    "                mdl_clone = clone(model)\n",
    "                mdl_clone.fit(X[train_idx], y[train_idx])\n",
    "                oof[val_idx] = mdl_clone.predict_proba(X[val_idx])[:, 1]\n",
    "            oof_preds[:, i] = oof\n",
    "\n",
    "        # Train meta-model on OOF predictions\n",
    "        self.meta_model.fit(oof_preds, y)\n",
    "\n",
    "        # Retrain base models on full dataset\n",
    "        self.fitted_base_models = [clone(m).fit(X, y) for m in self.base_models]\n",
    "\n",
    "        return self\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        X = np.asarray(X)\n",
    "        # Get predictions from fully trained base models\n",
    "        meta_features = np.column_stack([\n",
    "            m.predict_proba(X)[:, 1] for m in self.fitted_base_models\n",
    "        ])\n",
    "        return self.meta_model.predict_proba(meta_features)\n",
    "\n",
    "    def predict(self, X):\n",
    "        return (self.predict_proba(X)[:, 1] > 0.5).astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "4a9df1e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Acer\\AppData\\Roaming\\Python\\Python312\\site-packages\\xgboost\\training.py:183: UserWarning: [11:37:01] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "C:\\Users\\Acer\\AppData\\Roaming\\Python\\Python312\\site-packages\\xgboost\\training.py:183: UserWarning: [11:37:02] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 2932, number of negative: 1756\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000278 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1785\n",
      "[LightGBM] [Info] Number of data points in the train set: 4688, number of used features: 7\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.625427 -> initscore=0.512646\n",
      "[LightGBM] [Info] Start training from score 0.512646\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Acer\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\utils\\validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 2932, number of negative: 1756\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000291 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1785\n",
      "[LightGBM] [Info] Number of data points in the train set: 4688, number of used features: 7\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.625427 -> initscore=0.512646\n",
      "[LightGBM] [Info] Start training from score 0.512646\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Acer\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\utils\\validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 2932, number of negative: 1756\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000272 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1785\n",
      "[LightGBM] [Info] Number of data points in the train set: 4688, number of used features: 7\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.625427 -> initscore=0.512646\n",
      "[LightGBM] [Info] Start training from score 0.512646\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Acer\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\utils\\validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 2932, number of negative: 1756\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000313 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1785\n",
      "[LightGBM] [Info] Number of data points in the train set: 4688, number of used features: 7\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.625427 -> initscore=0.512646\n",
      "[LightGBM] [Info] Start training from score 0.512646\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Acer\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\utils\\validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 2932, number of negative: 1756\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000199 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1785\n",
      "[LightGBM] [Info] Number of data points in the train set: 4688, number of used features: 7\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.625427 -> initscore=0.512646\n",
      "[LightGBM] [Info] Start training from score 0.512646\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Acer\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\utils\\validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "C:\\Users\\Acer\\AppData\\Roaming\\Python\\Python312\\site-packages\\xgboost\\training.py:183: UserWarning: [11:37:29] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 3665, number of negative: 2195\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000305 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1785\n",
      "[LightGBM] [Info] Number of data points in the train set: 5860, number of used features: 7\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.625427 -> initscore=0.512646\n",
      "[LightGBM] [Info] Start training from score 0.512646\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Acer\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\utils\\validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "C:\\Users\\Acer\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\utils\\validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Define base learners\n",
    "rf = RandomForestClassifier(bootstrap= True, criterion= 'gini', max_depth= None, \n",
    "                            max_samples= 0.5, min_samples_leaf= 1, n_estimators= 150, oob_score=True)\n",
    "\n",
    "xgb = XGBClassifier(n_estimators=200, max_depth=5, learning_rate=0.1,\n",
    "                    subsample=1, colsample_bytree=1,\n",
    "                    eval_metric='logloss', use_label_encoder=False, random_state=42)\n",
    "\n",
    "gb = GradientBoostingClassifier(learning_rate= 0.5, max_depth= 5, n_estimators= 150,\n",
    "                                 subsample=1, random_state=42)\n",
    "\n",
    "\n",
    "lgb = LGBMClassifier(n_estimators=500, learning_rate=0.05,\n",
    "                     subsample=0.8, colsample_bytree=0.8, random_state=42)\n",
    "\n",
    "svc = SVC(C=2.0, kernel='rbf', probability=True, random_state=42)\n",
    "\n",
    "base_models = [rf, xgb, gb, lgb, svc]\n",
    "\n",
    "# Option 1: Logistic Regression as meta learner\n",
    "meta_log = LogisticRegression(penalty='l2', C=1.0, solver='lbfgs', max_iter=500, random_state=42)\n",
    "\n",
    "# Option 2: Random Forest as meta learnern\n",
    "meta_rf = RandomForestClassifier(n_estimators=200, max_depth=None, random_state=42)\n",
    "\n",
    "# Build stacking model (just swap meta model here)\n",
    "stack_clf = OOFStackingClassifier(base_models=base_models, meta_model=meta_rf, n_splits=5)\n",
    "\n",
    "# Fit and predict\n",
    "stack_clf.fit(X_train, y_train)\n",
    "y_pred = stack_clf.predict(X_test)\n",
    "y_pred_proba = stack_clf.predict_proba(X_test)[:, 1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "bbdd01eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.932469304229195"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d53e4cf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Acer\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\utils\\validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5205020920502093"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tess_df=pd.read_csv('testing_tess.csv')\n",
    "X_tess=tess_df.drop(columns=['koi_disposition'])\n",
    "y_tess=tess_df['koi_disposition']\n",
    "y_pred3=stack_clf.predict(X_tess)\n",
    "accuracy_score(y_tess,y_pred3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bb4a120",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
