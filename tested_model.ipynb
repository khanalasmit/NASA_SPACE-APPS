{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0b054dfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 9716 entries, 0 to 9715\n",
      "Data columns (total 8 columns):\n",
      " #   Column           Non-Null Count  Dtype  \n",
      "---  ------           --------------  -----  \n",
      " 0   koi_period       9716 non-null   float64\n",
      " 1   koi_duration     9716 non-null   float64\n",
      " 2   koi_depth        9716 non-null   float64\n",
      " 3   koi_model_snr    9716 non-null   float64\n",
      " 4   koi_prad         9716 non-null   float64\n",
      " 5   koi_steff        9716 non-null   float64\n",
      " 6   koi_srad         9716 non-null   float64\n",
      " 7   koi_disposition  9716 non-null   float64\n",
      "dtypes: float64(8)\n",
      "memory usage: 607.4 KB\n",
      "[0. 1.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Acer\\AppData\\Roaming\\Python\\Python312\\site-packages\\xgboost\\training.py:183: UserWarning: [13:52:33] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "C:\\Users\\Acer\\AppData\\Roaming\\Python\\Python312\\site-packages\\xgboost\\training.py:183: UserWarning: [13:52:34] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 3668, number of negative: 2549\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000507 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1785\n",
      "[LightGBM] [Info] Number of data points in the train set: 6217, number of used features: 7\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.589995 -> initscore=0.363945\n",
      "[LightGBM] [Info] Start training from score 0.363945\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Acer\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\utils\\validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 3668, number of negative: 2549\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000158 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1785\n",
      "[LightGBM] [Info] Number of data points in the train set: 6217, number of used features: 7\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.589995 -> initscore=0.363945\n",
      "[LightGBM] [Info] Start training from score 0.363945\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Acer\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\utils\\validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 3668, number of negative: 2550\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000190 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1785\n",
      "[LightGBM] [Info] Number of data points in the train set: 6218, number of used features: 7\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.589900 -> initscore=0.363553\n",
      "[LightGBM] [Info] Start training from score 0.363553\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Acer\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\utils\\validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 3668, number of negative: 2550\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000137 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1785\n",
      "[LightGBM] [Info] Number of data points in the train set: 6218, number of used features: 7\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.589900 -> initscore=0.363553\n",
      "[LightGBM] [Info] Start training from score 0.363553\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Acer\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\utils\\validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 3668, number of negative: 2550\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000167 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1785\n",
      "[LightGBM] [Info] Number of data points in the train set: 6218, number of used features: 7\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.589900 -> initscore=0.363553\n",
      "[LightGBM] [Info] Start training from score 0.363553\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Acer\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\utils\\validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "C:\\Users\\Acer\\AppData\\Roaming\\Python\\Python312\\site-packages\\xgboost\\training.py:183: UserWarning: [13:53:31] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 4585, number of negative: 3187\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000210 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1785\n",
      "[LightGBM] [Info] Number of data points in the train set: 7772, number of used features: 7\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.589938 -> initscore=0.363710\n",
      "[LightGBM] [Info] Start training from score 0.363710\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Acer\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\utils\\validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "C:\\Users\\Acer\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\utils\\validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8955761316872428"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ...existing code...\n",
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin, clone\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.svm import SVC\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score,f1_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "Kepler_data=pd.read_csv('Combined.csv')\n",
    "Kepler_data.info()\n",
    "X=Kepler_data.drop(columns=['koi_disposition'])\n",
    "y=Kepler_data['koi_disposition']\n",
    "le=LabelEncoder()\n",
    "le.fit(y)\n",
    "y_trf=le.transform(y)\n",
    "print(le.classes_)\n",
    "X_train,X_test,y_train,y_test=train_test_split(X,y_trf,test_size=0.2,random_state=42,stratify=y)\n",
    "\n",
    "class OOFStackingClassifier(BaseEstimator, ClassifierMixin):\n",
    "    \"\"\"\n",
    "    Out-Of-Fold stacking classifier that builds default base learners and a default meta-learner\n",
    "    if none are provided. Use base_models/meta_model kwargs to override.\n",
    "    \"\"\"\n",
    "    def __init__(self, base_models=None, meta_model=None, n_splits=5, random_state=42):\n",
    "        self.n_splits = n_splits\n",
    "        self.random_state = random_state\n",
    "\n",
    "        # default base learners\n",
    "        if base_models is None:\n",
    "            rf = RandomForestClassifier(bootstrap=True, criterion='gini', max_depth=None,\n",
    "                                        max_samples=0.8, min_samples_leaf=1, n_estimators=500,\n",
    "                                        oob_score=True, random_state=self.random_state)\n",
    "            xgb = XGBClassifier(n_estimators=200, max_depth=5, learning_rate=0.1,\n",
    "                                subsample=1, colsample_bytree=1,\n",
    "                                eval_metric='logloss', use_label_encoder=False, random_state=self.random_state)\n",
    "            gb = GradientBoostingClassifier(learning_rate=0.1, max_depth=3, n_estimators=500,\n",
    "                                            subsample=1, random_state=self.random_state)\n",
    "            lgb = LGBMClassifier(n_estimators=500, learning_rate=0.05,\n",
    "                                 subsample=0.8, colsample_bytree=0.8, random_state=self.random_state)\n",
    "            svc = SVC(C=2.0, kernel='rbf', probability=True, random_state=self.random_state)\n",
    "            base_models = [rf, xgb, gb, lgb, svc]\n",
    "\n",
    "        # default meta learner\n",
    "        if meta_model is None:\n",
    "            meta_model = RandomForestClassifier(n_estimators=200, max_depth=None, random_state=self.random_state)\n",
    "\n",
    "        self.base_models = base_models\n",
    "        self.meta_model = meta_model\n",
    "        self.fitted_base_models = []\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        X = np.asarray(X)\n",
    "        y = np.asarray(y)\n",
    "        n_samples = X.shape[0]\n",
    "\n",
    "        skf = StratifiedKFold(n_splits=self.n_splits, shuffle=True, random_state=self.random_state)\n",
    "        oof_preds = np.zeros((n_samples, len(self.base_models)))\n",
    "\n",
    "        # Out-of-fold predictions for each base model\n",
    "        for i, model in enumerate(self.base_models):\n",
    "            oof = np.zeros(n_samples)\n",
    "            for train_idx, val_idx in skf.split(X, y):\n",
    "                mdl_clone = clone(model)\n",
    "                mdl_clone.fit(X[train_idx], y[train_idx])\n",
    "                # try predict_proba, fallback to decision_function, fallback to predict\n",
    "                if hasattr(mdl_clone, \"predict_proba\"):\n",
    "                    oof[val_idx] = mdl_clone.predict_proba(X[val_idx])[:, 1]\n",
    "                elif hasattr(mdl_clone, \"decision_function\"):\n",
    "                    # scale decision_function to [0,1] via sigmoid-like mapping\n",
    "                    df = mdl_clone.decision_function(X[val_idx])\n",
    "                    oof[val_idx] = 1 / (1 + np.exp(-df))\n",
    "                else:\n",
    "                    oof[val_idx] = mdl_clone.predict(X[val_idx])\n",
    "            oof_preds[:, i] = oof\n",
    "\n",
    "        # Train meta-model on OOF predictions\n",
    "        self.meta_model.fit(oof_preds, y)\n",
    "\n",
    "        # Retrain base models on full dataset and save them\n",
    "        self.fitted_base_models = [clone(m).fit(X, y) for m in self.base_models]\n",
    "\n",
    "        return self\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        X = np.asarray(X)\n",
    "        # build meta-features from fully trained base models\n",
    "        meta_features = np.column_stack([\n",
    "            (m.predict_proba(X)[:, 1] if hasattr(m, \"predict_proba\")\n",
    "             else (1 / (1 + np.exp(-m.decision_function(X)))) if hasattr(m, \"decision_function\")\n",
    "             else m.predict(X))\n",
    "            for m in self.fitted_base_models\n",
    "        ])\n",
    "        # return meta-model probabilities if available, else wrap single-column scores\n",
    "        if hasattr(self.meta_model, \"predict_proba\"):\n",
    "            return self.meta_model.predict_proba(meta_features)\n",
    "        else:\n",
    "            probs = self.meta_model.predict(meta_features)\n",
    "            # ensure shape (n_samples, 2)\n",
    "            probs = np.vstack([1 - probs, probs]).T\n",
    "            return probs\n",
    "\n",
    "    def predict(self, X):\n",
    "        probs = self.predict_proba(X)\n",
    "        # assume binary prob in column 1\n",
    "        return (probs[:, 1] > 0.5).astype(int)\n",
    "\n",
    "# Example usage (keeps previous notebook variables/flow)\n",
    "# ...existing code...\n",
    "stack_clf = OOFStackingClassifier(n_splits=5, random_state=42)   # uses defaults defined above\n",
    "stack_clf.fit(X_train, y_train)\n",
    "y_pred = stack_clf.predict(X_test)\n",
    "y_pred_proba = stack_clf.predict_proba(X_test)[:, 1]\n",
    "\n",
    "accuracy_score(y_test,y_pred)\n",
    "# ...existing code..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e5d511d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ...existing code...\n",
    "import pickle\n",
    "\n",
    "# write using a file object\n",
    "with open(r'Stacked.pkl', 'wb') as f:\n",
    "    pickle.dump(stack_clf, f)\n",
    "# ...existing code..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5917c9ba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
